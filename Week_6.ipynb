{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP556, Ecole Polytechnique, 2022-23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC7 - Construction and training of feed-forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the construction of a multi-layer neural network in Python, using only basic libraries such as numpy.\n",
    "\n",
    "We will construct a NN $T_\\theta(x)$ and then, given some synthetic data $(x_i, y_i)_{i = 1, \\dots, N}$, solve the fitting (or training) problem\n",
    "\n",
    "$$\n",
    "\\min_\\theta R_N(\\theta) \\quad \\mbox{where} \\quad R_N(\\theta) = \\frac 1 N \\sum_{i=1}^N (y_i -  T_\\theta(x_i))^2\n",
    "$$\n",
    "\n",
    "applying a gradient descent algorithm\n",
    "\n",
    "\n",
    "$$\n",
    "\\theta_{n+1} = \\theta_n - \\eta \\, \\frac 1{\\left|B_{n+1} \\right|} \\sum_{i \\in B_{n+1}}^N \\nabla_\\theta \\, (y_i -  T_{\\theta_n}(x_i))^2 \n",
    "$$\n",
    "\n",
    "with step (or \"learning rate\") $\\eta$, where $(B_{n})_n$ is a sequence of batches.\n",
    "\n",
    "This procedure will require to implement the computation of the gradient $\\nabla_\\theta T(x_i)$, which can be done by backpropagation through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the notation from the lectures: a NN $T_\\theta$ is a composition of $L$ transformations $(T^l)_{l = 1, \\dots, L}$\n",
    "\n",
    "$$\n",
    "T_\\theta = T^L \\circ \\cdots \\circ T^1,\n",
    "$$\n",
    "\n",
    "where, for each $l$, $T^l : \\mathbb R^{d^l} \\to \\mathbb R^{d^{l+1}}$ is given by\n",
    "\n",
    "$$\n",
    "T^l(x) = \\sigma^l \\left(W^l x + b^l \\right).\n",
    "$$\n",
    "\n",
    "The matrices $W^l \\in \\mathbb R^{d^{l+1} \\times d^l}$ are called the network _weights_, the vectors $b^l \\in \\mathbb R^{d^{l+1}}$  the network _biases_.\n",
    "\n",
    "The activation function $\\sigma^l$ acts componentwise, $\\sigma^l(a) = (\\sigma^l(a_1), \\dots, \\sigma^l(a_{d^{l+1}}))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class implements the linear transformation part of a generic layer within the network (a separate class will be used to implement the activation function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Random initialisation of the layer weight parameters\n",
    "        # (note that, using the notation of the lectures, self.weights corresponds to W.T).\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        \n",
    "        # Random initialisation of the layer bias parameters b\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):        \n",
    "        # The input data is copied into the layer\n",
    "        self.input = input_data\n",
    "        \n",
    "        # Note that output is implemented as x * W instead of W * x\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # Recall the notation from the lectures :\n",
    "    # - the transformation from layer input x to layer output y is denoted T.\n",
    "    # The fct below computes dT/dW, dT/dB for a given output_error dT/dy. Returns input_error=dT/dX.\n",
    "    def backward_propagation(self, output_error):\n",
    "        self.bias_error = output_error\n",
    "        self.weights_error = np.dot(self.input.T, output_error) # shape = input_size * output_size\n",
    "        \n",
    "        self.input_error = np.dot(output_error, self.weights.T) # shape = 1 * input_size\n",
    "        \n",
    "        # We need to output the value of dT/dX so that it can be used by the following layer\n",
    "        # within the backpropagation algorithm\n",
    "        return self.input_error\n",
    "        \n",
    "    # perform one gradient step and update parameters\n",
    "    def gradient_step(self, learning_rate):\n",
    "        #print(self.weights,self.output)\n",
    "        #T = np.dot(self.weights,self.output) + self.bias\n",
    "        #print(T)\n",
    "        self.weights -= 2*learning_rate*np.dot(self.input[0]-self.output[0,:-1],self.input[0])\n",
    "        self.bias -= 2*learning_rate*sum(self.input[0]-self.output[0,:-1])\n",
    "        \n",
    "        print(self.weights,self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our FCLayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FClayer1.weights.shape :  (2, 3)\n",
      "FClayer1.weights : \n",
      " [[ 0.0947949   0.10842584  0.23225595]\n",
      " [ 0.19000863 -0.26796241  0.48755032]] \n",
      "\n",
      "FClayer1.bias.shape :  (1, 3)\n",
      "FClayer1.bias: \n",
      " [[0.47823077 0.27476162 0.3675975 ]] \n",
      "\n",
      "layer_output :  [[ 0.95304292 -0.15273737  1.57495409]] \n",
      "\n",
      "\n",
      "Input [[1 2]]\n",
      "Output [[ 0.95304292 -0.15273737  1.57495409]]\n",
      "\n",
      "derivative w.r.t. input :  [[1.00841443 1.11673477]] \n",
      "\n",
      "[[ 0.0947949   0.10842584  0.23225595]\n",
      " [ 0.19000863 -0.26796241  0.48755032]] [[ 0.95304292 -0.15273737  1.57495409]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vt/sdb7skj50blfnp70pjvzn9fw0000gn/T/ipykernel_9259/4243102231.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# test parameters update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mFClayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Updated FClayer1.weights : \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFClayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vt/sdb7skj50blfnp70pjvzn9fw0000gn/T/ipykernel_9259/3853205616.py\u001b[0m in \u001b[0;36mgradient_step\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "FClayer1 = FCLayer(2, 3)\n",
    "\n",
    "print(\"FClayer1.weights.shape : \", FClayer1.weights.shape)\n",
    "print(\"FClayer1.weights : \\n\", FClayer1.weights, \"\\n\")\n",
    "\n",
    "print(\"FClayer1.bias.shape : \", FClayer1.bias.shape)\n",
    "print(\"FClayer1.bias: \\n\", FClayer1.bias, \"\\n\")\n",
    "\n",
    "# test forward computation\n",
    "input_data = np.array([[1, 2]])\n",
    "layer_output = FClayer1.forward_propagation(input_data)\n",
    "print(\"layer_output : \", layer_output, \"\\n\")\n",
    "\n",
    "print()\n",
    "print('Input',FClayer1.input)\n",
    "print('Output',FClayer1.output)\n",
    "print()\n",
    "\n",
    "# test backward computation\n",
    "input_derivative = FClayer1.backward_propagation(np.array([[1, 2, 3]]))\n",
    "print(\"derivative w.r.t. input : \", input_derivative, \"\\n\")\n",
    "\n",
    "# test parameters update\n",
    "FClayer1.gradient_step(0.1)\n",
    "print(\"Updated FClayer1.weights : \\n\", FClayer1.weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Activation function__. The following class implements the activation function as an additional layer inside the NN (with input size = output size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        # The input data is copied into the layer\n",
    "        self.input = input_data\n",
    "        \n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # The learning_rate is not used because there is no \"learnable\" parameter within the activation function.\n",
    "    def backward_propagation(self, output_error):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the ActivationLayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output :  [[0.76159416 0.96402758]]\n",
      "derivative w.r.t. input :  [[0.41997434 0.14130165]]\n"
     ]
    }
   ],
   "source": [
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n",
    "\n",
    "ActivationLayer1 = ActivationLayer(tanh, tanh_prime)\n",
    "\n",
    "print(\"output : \", ActivationLayer1.forward_propagation( np.array([[1, 2]]) ) )\n",
    "\n",
    "print(\"derivative w.r.t. input : \" , ActivationLayer1.backward_propagation( np.array([[1, 2]]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_true-y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Full network and training functions__. Finally, we implement the full network, allowing for the possibility to pile up layers made by linear transformations and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # compute output for given input\n",
    "    def predict(self, input_data):\n",
    "        # detect the sample dimension first\n",
    "        sample_lenght = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(sample_lenght):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        sample_lenght = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(sample_lenght):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = 0. ### TO DO: UPDATE HERE  ### \n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)                \n",
    "                for layer in reversed(self.layers):\n",
    "                    error = 0. ### TO DO: UPDATE HERE  ###\n",
    "                    \n",
    "                    if hasattr(layer, 'gradient_step') and callable(getattr(layer, 'gradient_step')):\n",
    "                        pass ### TO DO: COMPLETE HERE WITH THE REQUIRED OPERATION ### \n",
    "\n",
    "            # calculate average error on all samples (for display purpose only)\n",
    "            err /= sample_lenght\n",
    "            if i % 100 == 0:\n",
    "                print('epoch %d/%d  error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the network training on some toy examples.\n",
    "\n",
    "__First example__. As a start, we fit a small NN with one hidden layer to a function $y(x)$ defined on the vertices of the square $[0,1]^2$\n",
    "\n",
    "(Of course this is a very simple example, and the NN will be able to perform well - but already in this basic situation, we will see the importance of hyperparameters such as the random initialisation of the network and the learning rate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape :  (4, 1, 2)\n",
      "len(x_train) :  4\n",
      "y_train.shape : (4, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "print(\"x_train.shape : \", x_train.shape)\n",
    "print(\"len(x_train) : \", len(x_train))\n",
    "\n",
    "print(\"y_train.shape :\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial network output y : [array([[0.23594761]]), array([[0.37076857]]), array([[0.1332286]]), array([[0.27580133]])] \n",
      "\n",
      "epoch 1/1000  error=0.500000\n",
      "epoch 101/1000  error=0.500000\n",
      "epoch 201/1000  error=0.500000\n",
      "epoch 301/1000  error=0.500000\n",
      "epoch 401/1000  error=0.500000\n",
      "epoch 501/1000  error=0.500000\n",
      "epoch 601/1000  error=0.500000\n",
      "epoch 701/1000  error=0.500000\n",
      "epoch 801/1000  error=0.500000\n",
      "epoch 901/1000  error=0.500000\n",
      "\n",
      " Trained network output y : [array([[0.23594761]]), array([[0.37076857]]), array([[0.1332286]]), array([[0.27580133]])]\n"
     ]
    }
   ],
   "source": [
    "# network\n",
    "input_size = x_train.shape[-1]\n",
    "output_size = y_train.shape[-1]\n",
    "\n",
    "net = Network()\n",
    "\n",
    "net.add(FCLayer(input_size, 3))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(3, output_size))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# starting point \n",
    "print(\"Initial network output y :\", net.predict(x_train), \"\\n\")\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# the resulting output\n",
    "result = net.predict(x_train)\n",
    "print(\"\\n Trained network output y :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Another example__.  Here we ask a NN to fit some noisy observations of the function $y(x) = \\sin(4 \\, x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training data\n",
    "n_sample = 200\n",
    "x_train = np.linspace(-1, 1, n_sample)\n",
    "y_train = np.sin(x_train*4) + np.random.uniform(-0.1, 0.1, n_sample)\n",
    "\n",
    "# plot the training data\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.title(\"Training data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,2) and (1,10) not aligned: 2 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vt/sdb7skj50blfnp70pjvzn9fw0000gn/T/ipykernel_9259/2431812637.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# starting point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0minitial_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initial_output.shape : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vt/sdb7skj50blfnp70pjvzn9fw0000gn/T/ipykernel_9259/1042917658.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vt/sdb7skj50blfnp70pjvzn9fw0000gn/T/ipykernel_9259/3574591657.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Note that output is implemented as x * W instead of W * x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,2) and (1,10) not aligned: 2 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# network\n",
    "nb_neurons = 10\n",
    "\n",
    "net = Network()\n",
    "\n",
    "net.add(FCLayer(1, nb_neurons))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(nb_neurons, 1))\n",
    "#net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# starting point \n",
    "initial_output = net.predict(x_train)\n",
    "print(\"initial_output.shape : \", np.array(initial_output).shape )\n",
    "\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(x_train, np.squeeze(initial_output), 'r')\n",
    "plt.title(\"Initial network output\")\n",
    "plt.show()\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# plot the resulting output\n",
    "result = net.predict(x_train)\n",
    "\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(x_train, np.squeeze(result), 'r')\n",
    "plt.title(\"Trained network output\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
